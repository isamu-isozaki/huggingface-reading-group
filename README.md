# Hugginface Reading Group
Welcome to the Huggingface Reading Group! The goal of this group is to have a weekly presentation on research papers/groups of papers. The goal of this repository is to compile all the past presentation write-ups and recordings.

## Brief History
This group was started by Huggingface community member James Kelly on 09/26/2023. In the beginning, we "presented" via a summary of papers in discord threads but we started 1/12/2024 to do presentations in discord calls thanks to Phil Butler. The presentations, in general, are targetted for the general audience on the subject of Generative Models but no research papers are off limits.

## 0: Ambiguity-Aware In-Context Learning with Large Language Models(Presented on 9/27/2023)
Presenter: James Kelly

Paper: [Ambiguity-Aware In-Context Learning with Large Language Models](https://arxiv.org/pdf/2309.07900.pdf)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1156474997540405248)


## 1: Controlling Neural Networks with Rule Representations(Presented on 10/05/2023)
Presenter: James Kelly

Paper: [Controlling Neural Networks with Rule Representations (NeurIPs, 2021)](https://arxiv.org/pdf/2106.07804.pdf)

[Code](https://github.com/googleinterns/controllabledl/tree/master)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1159576781863530677)

## 2: Understanding Instaflow/Rectified Flow(Presented on 10/11/2023)
Presenter: Isamu Isozaki

Paper: [InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation](https://arxiv.org/abs/2309.06380)

[Write up](https://huggingface.co/blog/Isamu136/insta-rectified-flow)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1161833150364786758)

## 3: Mysteries of Text Embeddings(Presented on 10/19/2023)
Presenter: Isamu Isozaki

Papers: [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816)+[NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)

[Discord Thread](https://discord.com/channels/879548962464493619/1164728400754245793/1164728404470411364)

## 4: Training Image Derivatives: Increased Accuracy and Universal Robustness(Presented on 11/08/2023)
Presenter: Vsevolod I. Avrutskiy. Author of the paper

Paper: [Training Image Derivatives: Increased Accuracy and Universal Robustness](https://arxiv.org/abs/2310.14045)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1171931030823911484)

## 5: Understanding Zephyr(Presented on 11/16/2023)
Presenter: Isamu Isozaki

Paper: [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)

[Write up](https://medium.com/@isamu-website/understanding-zephyr-12c5b9d3b822)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1174911781215408128)

##  6: Literature Review on RAG(Retrieval Augmented Generation) for Custom Domains(Presented on 11/29/2023)
Presenter: Isamu Isozaki

Papers: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) + [Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering](https://arxiv.org/abs/2210.02627) + [RA-DIT: Retrieval-Augmented Dual Instruction Tuning](https://arxiv.org/abs/2310.01352)

[Write up](https://medium.com/@isamu-website/literature-review-on-rag-retrieval-augmented-generation-for-custom-domains-325bcef98be4)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1179641601967001672)

## 7: Understanding MagVIT2: Language Model Beats Diffusion: Tokenizer is key to visual generation(Presented on 12/13/2023)
Presenter: Isamu Isozaki

Paper: [Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation](https://arxiv.org/abs/2310.05737)

[Write up](https://isamu-website.medium.com/understanding-magvit2-language-model-beats-diffusion-tokenizer-is-key-to-visual-generation-8adba03b724c)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1184676635144896633)

## 8: Understanding Common Diffusion Noise Schedules and Sample Steps are Flawed(Presented on 12/21/2023)
Presenter: Isamu Isozaki

Paper: [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/abs/2305.08891)

[Write up](https://isamu-website.medium.com/understanding-common-diffusion-noise-schedules-and-sample-steps-are-flawed-and-offset-noise-52a73ab4fded)

[Discord Thread](https://discord.com/channels/879548962464493619/1156269946427428974/1187304406841049108)

## 9: The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey(Presented on 1/5/2024)
Presenter: Dhruv Dhamani. Author of the paper

Paper: [The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey](https://arxiv.org/abs/2312.17601)

[Discord Thread](https://discord.com/channels/879548962464493619/1192849172014039221/1192849174677430332)

## 10: Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation(Presented on 1/12/2024)
Presenter: Phil Butler

Paper: [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation](https://mobile-aloha.github.io/resources/mobile-aloha.pdf)

[Write up](https://sota.beehiiv.com/p/mobile-aloha?utm_source=sota.beehiiv.com&utm_medium=newsletter&utm_campaign=mobile-aloha)

Unfortunately, no recordings but a coauthors came.

## 11: Literature Review on AI in Law(Presented on 2/2/2024)
Presenter: Isamu Isozaki

Papers: [On the acceptability of arguments and its fundamental role in non-monotonic reasoning, logic programming, and n-person games](https://www.ijcai.org/Proceedings/93-2/Papers/003.pdf)+[An Answer Set Programming Approach to Argumentative Reasoning in the ASPIC+ Framework](https://proceedings.kr.org/2020/63/kr2020-0063-lehtonen-et-al.pdf)+[HYPO’s legacy: introduction to the virtual special issue](https://link.springer.com/article/10.1007/s10506-017-9201-1)+[Induction of Defeasible Logic Theories in the Legal Domain](https://dl.acm.org/doi/10.1145/1047788.1047834)+[Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset](https://arxiv.org/abs/2207.00220)+[Large Language Models in Law: A Survey](https://arxiv.org/abs/2312.03718)+[The Smart Court - A New Pathway to Justice in China?](https://storage.googleapis.com/jnl-up-j-ijca-files/journals/1/articles/367/submission/proof/367-1-1754-2-10-20210311.pdf)

[Write up](https://medium.com/@isamu-website/literature-review-on-ai-in-law-7fe80e352c34)

[Recording](https://www.youtube.com/watch?v=RGdeGiCe0ig)

[Slides](slides/11)

## 12: A forthcoming decoder-only foundation model for time-series forecasting & further research(Presented on 2/9/2024)
Presenter: Tonic

Paper: [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688)

[Recording](https://youtu.be/_2SWu1SOcG0)

[Slides](slides/12)

## 13: Mamba: Linear-Time Sequence Modeling with Selective State Spaces
Presenter: Eric Auld

Paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

[Recording](https://www.youtube.com/watch?v=CWQuL8dpCRY)

## 14: Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures
Presenter: Vincent Abbott. Author of the paper

Paper: [Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures](https://arxiv.org/abs/2402.05424)

[Recording](https://youtu.be/pwM_PzqvF9U)

## 15: SOTA on Model Merging
Presenter: Prateek Yadav. Author of TIES-Merging and ComPEFT

Papers: [TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708)+[Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099)+[ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization](https://arxiv.org/abs/2311.13171)+[Learning to Route Among Specialized Experts for Zero-Shot Generalization](https://arxiv.org/abs/2402.05859)

[Recording](https://www.youtube.com/watch?v=OsAKLPhwG0Q&t=3s)

## 16: Gemini 1.5 Pro: Unlock reasoning and knowledge from entire books and movies in a single prompt
Presenter: Shashank Shekhar

Papers: [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) + [Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference](https://arxiv.org/abs/2110.03742) + [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) + [Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts](https://arxiv.org/abs/2206.02770)

[Recording](https://www.youtube.com/watch?v=IuehDA1M_Lw)

[Slides](https://link.excalidraw.com/p/readonly/16L28yECcUG802Ec2rmx)

## 17: HyperZ⋅Z⋅W Operator Connects Slow-Fast Networks for Full Context Interaction
Presenter: Harvie Zhang. Author of the paper

Paper: [HyperZ⋅Z⋅W Operator Connects Slow-Fast Networks for Full Context Interaction](https://arxiv.org/abs/2401.17948)

[Recording](https://www.youtube.com/watch?v=urgLoVPj1P8&t=23s&ab_channel=IsamuIsozaki)

## 18: ProteinBERT: A universal deep-learning model of protein sequence and function
Presenter: Dan Ofer. Author of the papers

Papers: [ProteinBERT: A universal deep-learning model of protein sequence and function](https://www.biorxiv.org/content/10.1101/2021.05.24.445464v1)+[Detecting anomalous proteins using deep representations](https://academic.oup.com/nargab/article/6/1/lqae021/7614821)+[Protein Language Models Expose Viral Mimicry and Immune Escape](https://www.biorxiv.org/content/biorxiv/early/2024/03/15/2024.03.14.585057.full.pdf)

[Recording](https://youtu.be/c_Fx-eFfFB0)

[Slides](https://docs.google.com/presentation/d/1JqF0pZHHWieu4-w1d31HYux3rBkr21qh7mRxGnJGupo/edit#slide=id.ge53ec7dbea_0_0)

## 19: Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
I was absent this meeting so if anyone knows, please let me know/do a pr to fill this part!

Paper: [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853)

## 20: Graph Machine Learning in the Era of Large Language Models (LLMs)
Presenter: Isamu Isozaki

Papers: [Graph Machine Learning in the Era of Large Language Models (LLMs)](https://arxiv.org/abs/2404.14928)+[Large Language Models on Graphs: A Comprehensive Survey](https://arxiv.org/abs/2312.02783)+[House-GAN: Relational Generative Adversarial Networks for Graph-constrained House Layout Generation](https://arxiv.org/abs/2003.06988)

[Recording](https://www.youtube.com/watch?v=cgMAvqgq0Ew&ab_channel=IsamuIsozaki)

[Write up](https://medium.com/@isamu-website/understanding-graph-machine-learning-in-the-era-of-large-language-models-llms-dce2fd3f3af4)

[Slides](slides/20)

## 21: Story Generation with AI
Presenter: Isamu Isozaki

Papers: [GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence](https://arxiv.org/pdf/2310.05388)+[Creating Suspenseful Stories: Iterative Planning with Large Language Models](https://arxiv.org/pdf/2402.17119)+[Improving Pacing in Long-Form Story Planning](https://arxiv.org/pdf/2311.04459)+[Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives](https://www.arxiv.org/abs/2402.11051)+[Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers](https://arxiv.org/pdf/2403.01061)+[DOC: Improving Long Story Coherence With Detailed Outline Control](https://arxiv.org/pdf/2212.10077)+[End-to-end Story Plot Generator](https://arxiv.org/pdf/2310.08796)+[Weaver: Foundation Models for Creative Writing](https://arxiv.org/pdf/2401.17268)

[Recording](https://www.youtube.com/watch?v=UvWVfVnVZXc&ab_channel=IsamuIsozaki)

[Write up](https://medium.com/@isamu-website/understanding-ai-for-stories-d0c1cd7b7bdc)

[Slides](slides/21)

## 22: AlphaFold 3
Presnter: starrynightdev

Papers: [Accurate structure prediction of biomolecular interactions with AlphaFold 3](https://doi.org/10.1038/s41586-024-07487-w)+[Highly accurate protein structure prediction with AlphaFold](https://doi.org/10.1038/s41586-021-03819-2)

[Recording](https://youtu.be/qndfx4cdUZk)

Write ups: [Huggingface blog](https://huggingface.co/blog/as-cle-bert/what-is-going-on-with-alphafold3)+[Github blog](https://astrabert.github.io/hophop-science/AI-predicts-proteins-and-edits-DNA/)

[Slides](https://docs.google.com/presentation/d/1Q5GIE0-YsupEBvXRddoBsfVCWr9ewvcu/edit?usp=sharing&ouid=115858478176864292108&rtpof=true&sd=true)

## 23: AI for Physics. Hamilton Neural Networks/Lagrangian Neural Networks
Presenter: PS_Venom

Papers: [Hamiltonian Neural Networks](https://arxiv.org/pdf/1906.01563)+[Lagrangian Neural Networks](https://arxiv.org/abs/2003.04630)

[Recording](https://www.youtube.com/watch?v=rVw4Zipmo1I&ab_channel=IsamuIsozaki)

[Slides](https://www.canva.com/design/DAGHa7wgn_U/3JRcQJTblrLhztmJUMUjPg/view)

## 24: Understanding Current State of Reasoning with LLMs
Presenter: Isamu Isozaki

Papers: [Natural Language Reasoning, A Survey](https://arxiv.org/abs/2303.14725) + [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682) + [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) + [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) + [Show Your Work: Scratchpads for Intermediate Computation with Language Models](https://arxiv.org/abs/2112.00114) + [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221) + [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601) + [Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2305.16582)+[Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought](https://arxiv.org/abs/2308.08614) + [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001) + [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093) + [Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://arxiv.org/abs/2305.04388) + [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798) + [The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925) + [Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning](https://arxiv.org/abs/2305.12295) + [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/abs/2401.17464) + [Self-playing Adversarial Language Game Enhances LLM Reasoning](https://arxiv.org/abs/2404.10642)

[Recording](https://www.youtube.com/watch?v=vbji1PvXgBc&ab_channel=IsamuIsozaki)

[Slides](slides/24)

[Write up](https://medium.com/@isamu-website/understanding-the-current-state-of-reasoning-with-llms-dbd9fa3fc1a0)

## 25:  Multimodal Structured Generation & CVPR’s 2nd MMFM Challenge
Presenter: Franz Louis Cesista. Author of paper

Paper: [Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report](https://arxiv.org/abs/2406.11403)

[Recording](https://www.youtube.com/watch?v=Of2BGbciOWI)

[Slides](https://docs.google.com/presentation/d/1UuPaLxesik7zEjVDP3V8tnjidX68iH4ZCUr_MpBY5Aw/edit)

## 26: SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound
Presenter: Rishit Dagli. First author of paper

Paper: [SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound](https://arxiv.org/abs/2406.06612)

[Recording](https://youtu.be/7wkgFR-HYjY)

## 27: Understanding Penetration Testing with LLMs
Presenter: Isamu Isozaki, Manil Shrestha

Papers: [PentestGPT: An LLM-empowered Automatic Penetration Testing Tool](https://arxiv.org/abs/2308.06782)+[LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664)+[LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org/abs/2404.08144)+[Teams of LLM Agents can Exploit Zero-Day Vulnerabilities](https://arxiv.org/abs/2406.01637)+[LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks](https://arxiv.org/abs/2310.11409)+[AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks](https://arxiv.org/abs/2403.01038)

[Recording](https://youtu.be/_f16ofdVC8g)

[Slides](https://docs.google.com/presentation/d/1OF_wqUsbbsFoAu4XZFlcaaOmnZngrUuvQrTstcogZ5c/edit)

[Write up](https://medium.com/gopenai/understanding-penetration-testing-with-llms-2b0ec6add14a)
